{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ca0a21",
   "metadata": {},
   "source": [
    "# Modeling Notebook\n",
    "\n",
    "This notebook demonstrates training and evaluating machine learning models for stress detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our modules\n",
    "from models import StressDetectionModel, compare_models, leave_one_subject_out_cv\n",
    "from utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e997e975",
   "metadata": {},
   "source": [
    "## Load Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cf072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "processed_dir = '../data/processed'\n",
    "features_path = os.path.join(processed_dir, 'extracted_features.csv')\n",
    "\n",
    "if os.path.exists(features_path):\n",
    "    features_df = load_dataset(features_path)\n",
    "    print(\"Features loaded successfully!\")\n",
    "    print(f\"Features shape: {features_df.shape}\")\n",
    "    print(features_df.head())\n",
    "else:\n",
    "    print(\"Processed features not found. Creating sample data for demonstration.\")\n",
    "    \n",
    "    # Create sample features for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    \n",
    "    features_df = pd.DataFrame({\n",
    "        'mean_rr': np.random.normal(800, 50, n_samples),\n",
    "        'sdnn': np.random.normal(50, 10, n_samples),\n",
    "        'rmssd': np.random.normal(30, 15, n_samples),\n",
    "        'pnn50': np.random.normal(10, 5, n_samples),\n",
    "        'lf_hf_ratio': np.random.normal(1.5, 0.5, n_samples),\n",
    "        'mean_eda': np.random.normal(6, 2, n_samples),\n",
    "        'std_eda': np.random.normal(1.5, 0.5, n_samples),\n",
    "        'num_scr_peaks': np.random.poisson(5, n_samples),\n",
    "        'mean_acc_magnitude': np.random.normal(10, 2, n_samples),\n",
    "        'std_acc_magnitude': np.random.normal(1, 0.3, n_samples),\n",
    "        'label': np.random.choice([0, 1], size=n_samples, p=[0.6, 0.4])  # 60% no stress, 40% stress\n",
    "    })\n",
    "    \n",
    "    print(\"Sample features created successfully!\")\n",
    "    print(f\"Features shape: {features_df.shape}\")\n",
    "    print(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61641d",
   "metadata": {},
   "source": [
    "## Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de77e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "feature_columns = [col for col in features_df.columns if col not in ['label', 'window_id']]\n",
    "X = features_df[feature_columns]\n",
    "y = features_df['label']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Label vector shape: {y.shape}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1cb7e",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"Training class distribution:\\n{y_train.value_counts()}\")\n",
    "print(f\"Testing class distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b652c",
   "metadata": {},
   "source": [
    "## Train Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "rf_model = StressDetectionModel('rf')\n",
    "rf_model.train(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "rf_metrics = rf_model.evaluate(X_test, y_test)\n",
    "print(\"Random Forest Model Evaluation:\")\n",
    "print(f\"Accuracy: {rf_metrics['accuracy']:.3f}\")\n",
    "print(f\"Precision: {rf_metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {rf_metrics['recall']:.3f}\")\n",
    "print(f\"F1-Score: {rf_metrics['f1_score']:.3f}\")\n",
    "\n",
    "if 'roc_auc' in rf_metrics:\n",
    "    print(f\"ROC AUC: {rf_metrics['roc_auc']:.3f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\\n{np.array(rf_metrics['confusion_matrix'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58315a92",
   "metadata": {},
   "source": [
    "## Compare Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e03ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different models\n",
    "comparison_results = compare_models(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Model Comparison Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, metrics in comparison_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"  F1-Score: {metrics['f1_score']:.3f}\")\n",
    "    if 'roc_auc' in metrics:\n",
    "        print(f\"  ROC AUC: {metrics['roc_auc']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439a9f4",
   "metadata": {},
   "source": [
    "## Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c05c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "model_names = list(comparison_results.keys())\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "# Extract metric values\n",
    "metric_values = {metric: [comparison_results[model][metric] for model in model_names] for metric in metrics}\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i*width, metric_values[metric], width, label=metric.capitalize())\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(model_names)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812de5b",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876109fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(\"Performing hyperparameter tuning for Random Forest...\")\n",
    "rf_tuned = StressDetectionModel('rf')\n",
    "\n",
    "# Note: This would take some time to run\n",
    "# tuning_results = rf_tuned.hyperparameter_tuning(X_train, y_train, param_grid, cv=3)\n",
    "# print(f\"Best parameters: {tuning_results['best_params']}\")\n",
    "# print(f\"Best cross-validation score: {tuning_results['best_score']:.3f}\")\n",
    "\n",
    "print(\"Hyperparameter tuning example shown (commented out for faster execution).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed01397",
   "metadata": {},
   "source": [
    "## Leave-One-Subject-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7449bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of leave-one-subject-out cross-validation\n",
    "# This requires subject IDs for each sample\n",
    "print(\"Leave-One-Subject-Out Cross-Validation Example:\")\n",
    "\n",
    "# Create artificial subject IDs for demonstration\n",
    "subjects = np.random.choice(['S01', 'S02', 'S03', 'S04', 'S05'], size=len(X))\n",
    "subject_ids = pd.Series(subjects)\n",
    "\n",
    "print(f\"Number of unique subjects: {subject_ids.nunique()}\")\n",
    "print(f\"Subject distribution:\\n{subject_ids.value_counts()}\")\n",
    "\n",
    "# Note: This would take some time to run\n",
    "# loo_results = leave_one_subject_out_cv(X, y, subject_ids, model_type='rf')\n",
    "# print(f\"LOO CV Average Accuracy: {loo_results['average_scores']['accuracy']:.3f}\")\n",
    "# print(f\"LOO CV Average F1-Score: {loo_results['average_scores']['f1_score']:.3f}\")\n",
    "\n",
    "print(\"Leave-one-subject-out CV example shown (commented out for faster execution).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93fae5",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06764c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "model_dir = '../models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# For demonstration, we'll save the RF model\n",
    "# In practice, you would save the best performing model based on your evaluation\n",
    "model_path = os.path.join(model_dir, 'stress_detection_rf.pkl')\n",
    "# rf_model.save_model(model_path)\n",
    "# print(f\"Model saved to {model_path}\")\n",
    "\n",
    "print(\"Model saving example shown (commented out).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a306d",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from Random Forest\n",
    "if hasattr(rf_model.model, 'feature_importances_'):\n",
    "    importances = rf_model.model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance_df.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 10 Feature Importances (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a4540",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be6b92",
   "metadata": {},
   "source": [
    "In this notebook, we have:\n",
    "1. Loaded and prepared the feature data\n",
    "2. Trained and evaluated individual models\n",
    "3. Compared multiple machine learning algorithms\n",
    "4. Demonstrated hyperparameter tuning\n",
    "5. Showed cross-validation techniques\n",
    "6. Analyzed feature importances\n",
    "\n",
    "The next step would be to deploy the best performing model in a real-time application, which is demonstrated in the Streamlit app."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
